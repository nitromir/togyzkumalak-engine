"""
Gemini Integration for Togyzkumalak Analysis.

Uses the new google-genai SDK with gemini-3-flash-preview model.
Provides LLM-powered move analysis and position evaluation.
"""

import os
import asyncio
from typing import Dict, List, Optional

from .config import gemini_config


class GeminiAnalyzer:
    """
    Analyzes Togyzkumalak positions using Google's Gemini API.
    
    Features:
    - Position evaluation with explanation
    - Best move suggestion with reasoning
    - Game commentary
    - Strategic insights
    """
    
    # Kazakh hole names for better explanation
    HOLE_NAMES = {
        1: "–ê—Ä—Ç",
        2: "–¢–µ–∫—Ç“±—Ä–º–∞—Å", 
        3: "–ê—Ç ”©—Ç–ø–µ—Å",
        4: "–ê—Ç—Å—ã—Ä–∞—Ç–∞—Ä",
        5: "–ë–µ–ª",
        6: "–ë–µ–ª–±–∞—Å–∞—Ä",
        7: "“ö–∞–Ω–¥—ã“õ–∞“õ–ø–∞–Ω",
        8: "–ö”©–∫–º–æ–π—ã–Ω",
        9: "–ú–∞“£–¥–∞–π"
    }
    
    # Model display names in Russian
    MODEL_NAMES = {
        "polynet": "–ü–æ–ª–∏–ù–µ—Ç (–±–∞–∑–æ–≤–∞—è —Å–µ—Ç—å)",
        "alphazero": "–ê–ª—å—Ñ–∞–ó–µ—Ä–æ (MCTS)",
        "probs": "–ü–†–û–ë–° (Beam Search)"
    }
    
    # System prompt for consistent persona
    SYSTEM_PROMPT = """–¢—ã ‚Äî –≥—Ä–æ—Å—Å–º–µ–π—Å—Ç–µ—Ä –∏ —Ç—Ä–µ–Ω–µ—Ä –ø–æ –¢–æ–≥—ã–∑ –ö—É–º–∞–ª–∞–∫—É (–¢–æ“ì—ã–∑ “ö“±–º–∞–ª–∞“õ) —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∑–∏—Ü–∏–π –∏ –æ–±—ä—è—Å–Ω—è—Ç—å —Ç–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω—é–∞–Ω—Å—ã –∏–≥—Ä—ã.

–ü–†–ê–í–ò–õ–ê:
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ò—Å–ø–æ–ª—å–∑—É–π –∫–∞–∑–∞—Ö—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ª—É–Ω–æ–∫ (–ê—Ä—Ç, –¢–µ–∫—Ç“±—Ä–º–∞—Å, –ê—Ç ”©—Ç–ø–µ—Å, –ê—Ç—Å—ã—Ä–∞—Ç–∞—Ä, –ë–µ–ª, –ë–µ–ª–±–∞—Å–∞—Ä, “ö–∞–Ω–¥—ã“õ–∞“õ–ø–∞–Ω, –ö”©–∫–º–æ–π—ã–Ω, –ú–∞“£–¥–∞–π)
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω: —É–∫–∞–∑—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞ –ª—É–Ω–æ–∫ –∏ —Ç–æ—á–Ω—ã–µ —Ä–∞—Å—á—ë—Ç—ã
- –û–±—ä—è—Å–Ω—è–π –ª–æ–≥–∏–∫—É, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–Ω—Å—Ç–∞—Ç–∏—Ä—É–π —Ñ–∞–∫—Ç—ã
- –£—á–∏—Ç—ã–≤–∞–π –¥–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –∏—Ö –æ—Ü–µ–Ω–∏–≤–∞–π"""

    
    def __init__(self):
        self.client = None
        self.model = gemini_config.model
        self._init_client()
    
    def _init_client(self):
        """Initialize Gemini client if API key is available."""
        api_key = gemini_config.api_key or os.environ.get("GEMINI_API_KEY")
        
        if not api_key:
            print("[WARNING] Gemini API key not configured. Analysis will be unavailable.")
            return
        
        try:
            from google import genai
            self.client = genai.Client(api_key=api_key)
            print(f"[OK] Gemini client initialized successfully (model: {self.model})")
        except ImportError:
            print("[WARNING] google-genai package not installed. Run: pip install google-genai")
        except Exception as e:
            print(f"[ERROR] Failed to initialize Gemini client: {e}")

    def _build_generate_config(self, max_output_tokens: int, temperature: float):
        """
        Build a GenerateContentConfig for google-genai.
        Passing a plain dict may be ignored by some SDK versions, leading to short/truncated outputs.
        """
        try:
            from google.genai import types
            return types.GenerateContentConfig(
                max_output_tokens=int(max_output_tokens),
                temperature=float(temperature),
            )
        except Exception:
            # Fallback to dict for older SDKs
            return {
                "max_output_tokens": int(max_output_tokens),
                "temperature": float(temperature),
            }

    def _response_to_text(self, response) -> str:
        """
        Extract full text from google-genai response across SDK variants.
        Some versions expose `.text`, others require joining candidate parts.
        """
        if response is None:
            return ""

        text = getattr(response, "text", None)
        if isinstance(text, str) and text.strip():
            return text

        # Try candidates/parts structure
        try:
            candidates = getattr(response, "candidates", None) or []
            chunks: List[str] = []
            for cand in candidates:
                content = getattr(cand, "content", None)
                parts = getattr(content, "parts", None) or []
                for part in parts:
                    part_text = getattr(part, "text", None)
                    if isinstance(part_text, str) and part_text:
                        chunks.append(part_text)
            joined = "".join(chunks)
            return joined
        except Exception:
            return ""
    
    def _format_position(self, board_state: Dict) -> str:
        """Format board position for LLM consumption - fully in Russian."""
        lines = []
        lines.append("‚ïê‚ïê‚ïê –¢–ï–ö–£–©–ê–Ø –ü–û–ó–ò–¶–ò–Ø ‚ïê‚ïê‚ïê")
        lines.append("")
        
        # Black side (top) - reverse order for display
        black_pits = board_state.get("black_pits", [9]*9)
        black_kazan = board_state.get('black_kazan', 0)
        lines.append(f"–ß–Å–†–ù–´–ï (“ö–∞—Ä–∞/“ö–æ—Å—Ç–∞—É—à—ã):")
        lines.append(f"  –õ—É–Ω–∫–∏ [9‚Üê1]: {list(reversed(black_pits))}")
        lines.append(f"  –ö–∞–∑–∞–Ω: {black_kazan} –∫—É–º–∞–ª–∞–∫–æ–≤")
        
        # White side (bottom)
        white_pits = board_state.get("white_pits", [9]*9)
        white_kazan = board_state.get('white_kazan', 0)
        lines.append("")
        lines.append(f"–ë–ï–õ–´–ï (–ê“õ/–ë–∞—Å—Ç–∞—É—à—ã):")
        lines.append(f"  –õ—É–Ω–∫–∏ [1‚Üí9]: {white_pits}")
        lines.append(f"  –ö–∞–∑–∞–Ω: {white_kazan} –∫—É–º–∞–ª–∞–∫–æ–≤")
        
        # Material balance
        lines.append("")
        diff = white_kazan - black_kazan
        if diff > 0:
            lines.append(f"üìä –ú–∞—Ç–µ—Ä–∏–∞–ª: –±–µ–ª—ã–µ +{diff}")
        elif diff < 0:
            lines.append(f"üìä –ú–∞—Ç–µ—Ä–∏–∞–ª: —á—ë—Ä–Ω—ã–µ +{abs(diff)}")
        else:
            lines.append("üìä –ú–∞—Ç–µ—Ä–∏–∞–ª: —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ")
        
        # Victory progress
        lines.append(f"   –î–æ –ø–æ–±–µ–¥—ã: –±–µ–ª—ã–º –Ω—É–∂–Ω–æ {82 - white_kazan}, —á—ë—Ä–Ω—ã–º –Ω—É–∂–Ω–æ {82 - black_kazan}")
        
        # Tuzduk info
        white_tuzduk = board_state.get("white_tuzduk", 0)
        black_tuzduk = board_state.get("black_tuzduk", 0)
        if white_tuzduk > 0 or black_tuzduk > 0:
            lines.append("")
            lines.append("üè¥ –¢–£–ó–î–´–ö–ò:")
        if white_tuzduk > 0:
            lines.append(f"  ‚Ä¢ –£ –±–µ–ª—ã—Ö —Ç—É–∑–¥—ã–∫ –Ω–∞ –ª—É–Ω–∫–µ {white_tuzduk} —á—ë—Ä–Ω—ã—Ö ({self.HOLE_NAMES.get(white_tuzduk, '')})")
        if black_tuzduk > 0:
            lines.append(f"  ‚Ä¢ –£ —á—ë—Ä–Ω—ã—Ö —Ç—É–∑–¥—ã–∫ –Ω–∞ –ª—É–Ω–∫–µ {black_tuzduk} –±–µ–ª—ã—Ö ({self.HOLE_NAMES.get(black_tuzduk, '')})")
        
        # Current player and legal moves
        lines.append("")
        current = board_state.get("current_player", "white")
        current_ru = "–ë–ï–õ–´–ï" if current == "white" else "–ß–Å–†–ù–´–ï"
        lines.append(f"üéØ –•–æ–¥: {current_ru}")
        
        legal = board_state.get("legal_moves", list(range(9)))
        legal_with_names = [f"{m+1} ({self.HOLE_NAMES.get(m+1, '')})" for m in legal]
        lines.append(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ö–æ–¥—ã: {', '.join(legal_with_names)}")
        
        return "\n".join(lines)
    
    def _format_move_history(self, moves: List[Dict]) -> str:
        """Format move history for context - in Russian."""
        if not moves:
            return "–ò—Å—Ç–æ—Ä–∏—è —Ö–æ–¥–æ–≤: –ø–∞—Ä—Ç–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–∞—Å—å."
        
        lines = ["üìú –ò–°–¢–û–†–ò–Ø –•–û–î–û–í (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 20):"]
        # Get last 20 moves
        recent_moves = moves[-20:] if len(moves) > 20 else moves
        start_idx = len(moves) - len(recent_moves)
        
        for i, move in enumerate(recent_moves):
            move_num = start_idx + i + 1
            player = move.get("player", "?")
            player_ru = "–ë" if player.lower().startswith("w") else "–ß"
            notation = move.get("notation", str(move.get("move", "?")))
            lines.append(f"  {move_num}. {player_ru}: –ª—É–Ω–∫–∞ {notation}")
        
        return "\n".join(lines)
    
    def _format_ai_data(self, model_probs: Dict[str, Dict[int, float]]) -> str:
        """Format AI model probabilities for LLM consumption - in Russian."""
        if not model_probs:
            return ""
        
        lines = ["", "ü§ñ –û–¶–ï–ù–ö–ò –ù–ï–ô–†–û–°–ï–¢–ï–ô:"]
        for model_name, probs in model_probs.items():
            if not probs:
                continue
            
            # Get display name in Russian
            display_name = self.MODEL_NAMES.get(model_name, model_name)
            
            # Get top 3 moves for each model
            sorted_moves = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]
            if not sorted_moves or sorted_moves[0][1] < 0.01:
                lines.append(f"  ‚Ä¢ {display_name}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                continue
                
            moves_parts = []
            for m, p in sorted_moves:
                if p > 0.01:  # Only show moves with >1% probability
                    hole_name = self.HOLE_NAMES.get(m + 1, "")
                    moves_parts.append(f"–ª—É–Ω–∫–∞ {m+1} ({hole_name}) ‚Äî {p*100:.0f}%")
            
            if moves_parts:
                lines.append(f"  ‚Ä¢ {display_name}:")
                lines.append(f"    –¢–æ–ø —Ö–æ–¥—ã: {', '.join(moves_parts)}")
        
        # Add consensus note if models agree
        if len(model_probs) >= 2:
            top_moves = []
            for probs in model_probs.values():
                if probs:
                    best = max(probs.items(), key=lambda x: x[1])
                    if best[1] > 0.2:  # Only count if confident
                        top_moves.append(best[0])
            
            if len(top_moves) >= 2 and len(set(top_moves)) == 1:
                agreed_move = top_moves[0] + 1
                lines.append(f"  ‚ö° –ö–æ–Ω—Å–µ–Ω—Å—É—Å: –≤—Å–µ —Å–µ—Ç–∏ –≤—ã–±–∏—Ä–∞—é—Ç –ª—É–Ω–∫—É {agreed_move} ({self.HOLE_NAMES.get(agreed_move, '')})")
        
        return "\n".join(lines)

    def _build_analysis_prompt(self, position_text: str, history_text: str, ai_data_text: str = "") -> str:
        """Build the analysis prompt - optimized for CONCISE Russian output (max 3 paragraphs)."""
        
        # Build the user message with all context
        user_message = f"""{position_text}
{ai_data_text}

‚ïê‚ïê‚ïê –ó–ê–î–ê–ù–ò–ï ‚ïê‚ïê‚ïê
–î–∞–π –ö–†–ê–¢–ö–ò–ô –∞–Ω–∞–ª–∏–∑ –ø–æ–∑–∏—Ü–∏–∏ (–ú–ê–ö–°–ò–ú–£–ú 3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–±–∑–∞—Ü–∞!).

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:

**–û—Ü–µ–Ω–∫–∞:** [—á–∏—Å–ª–æ –æ—Ç -5 –¥–æ +5] ‚Äî [–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ—á–µ–º—É]

**–ü–æ–∑–∏—Ü–∏—è:** [2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ —Ç–æ–º, –∫—Ç–æ –≤–ª–∞–¥–µ–µ—Ç –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–æ–π, –≥–ª–∞–≤–Ω—ã–µ —É–≥—Ä–æ–∑—ã –∏ –∫–ª—é—á–µ–≤—ã–µ –ª—É–Ω–∫–∏]

**–°–æ–≤–µ—Ç:** [1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ —Ç–∞–∫—Ç–∏–∫–µ –¥–ª—è —Ö–æ–¥—è—â–µ–≥–æ –∏–≥—Ä–æ–∫–∞, –ù–û –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ö–æ–¥–∞]

‚õî –°–¢–†–û–ì–û: –ù–µ –¥–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ö–æ–¥! –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ø–∏—Å–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ ##. –ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –∫–∞–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ç–æ—Ä –º–∞—Ç—á–∞."""

        return f"{self.SYSTEM_PROMPT}\n\n{user_message}"

    def _build_suggest_prompt(self, position_text: str, legal_moves: List[int], ai_data_text: str = "") -> str:
        """Build the move suggestion prompt - CONCISE (max 3 paragraphs)."""
        
        user_message = f"""{position_text}
{ai_data_text}

‚ïê‚ïê‚ïê –ó–ê–î–ê–ù–ò–ï ‚ïê‚ïê‚ïê
–ü–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–π –ª—É—á—à–∏–π —Ö–æ–¥. –ú–ê–ö–°–ò–ú–£–ú 3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–±–∑–∞—Ü–∞!

–§–û–†–ú–ê–¢:

**–õ—É—á—à–∏–π —Ö–æ–¥: –ª—É–Ω–∫–∞ [N]** ‚Äî [–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ—á–µ–º—É —ç—Ç–æ —Å–∏–ª—å–Ω–µ–π—à–∏–π —Ö–æ–¥]

**–ò–¥–µ—è:** [2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –∫—É–¥–∞ –ø—Ä–∏–∑–µ–º–ª–∏—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫—É–º–∞–ª–∞–∫, –±—É–¥–µ—Ç –ª–∏ –∑–∞—Ö–≤–∞—Ç, —á—Ç–æ —ç—Ç–æ –¥–∞—ë—Ç]

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** –õ—É–Ω–∫–∞ [X] —Ç–æ–∂–µ –Ω–µ–ø–ª–æ—Ö–∞ ‚Äî [–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ]

‚õî –°–¢–†–û–ì–û: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ø–∏—Å–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ ##. –ü–∏—à–∏ –∂–∏–≤—ã–º —è–∑—ã–∫–æ–º, –∫–∞–∫ —Ç—Ä–µ–Ω–µ—Ä –¥–∞—ë—Ç —Å–æ–≤–µ—Ç —É—á–µ–Ω–∏–∫—É."""

        return f"{self.SYSTEM_PROMPT}\n\n{user_message}"

    async def analyze_position_stream(
        self,
        board_state: Dict,
        move_history: List[Dict] = None,
        model_probs: Dict[str, Dict[int, float]] = None
    ):
        """
        Analyze the current position and yield text chunks (streaming).
        Uses a queue to properly async iterate over synchronous Gemini stream.
        """
        print(f"[Gemini Analysis] Starting position analysis")
        if not self.client:
            print(f"[Gemini Analysis] Client not available")
            yield "Gemini not configured."
            return
        
        position_text = self._format_position(board_state)
        history_text = self._format_move_history(move_history or [])
        ai_data_text = self._format_ai_data(model_probs or {})
        prompt = self._build_analysis_prompt(position_text, history_text, ai_data_text)
        
        try:
            gen_cfg = self._build_generate_config(
                max_output_tokens=4000,  # Increased for comprehensive analysis: ~20 paragraphs
                temperature=0.6,
            )

            # Use async queue to bridge sync stream to async generator
            import queue
            import threading
            
            chunk_queue: queue.Queue = queue.Queue()
            error_container = {"error": None}
            
            def stream_worker():
                """Worker thread to consume sync stream and put chunks in queue."""
                try:
                    response_stream = self.client.models.generate_content_stream(
                        model=self.model,
                        contents=prompt,
                        config=gen_cfg
                    )
                    for chunk in response_stream:
                        text = self._response_to_text(chunk)
                        if text:
                            chunk_queue.put(text)
                except Exception as e:
                    error_container["error"] = str(e)
                finally:
                    chunk_queue.put(None)  # Signal end of stream
            
            # Start worker thread
            worker = threading.Thread(target=stream_worker, daemon=True)
            worker.start()
            
            # Async consume from queue
            while True:
                # Non-blocking check with short sleep to yield control
                try:
                    chunk = chunk_queue.get_nowait()
                except queue.Empty:
                    await asyncio.sleep(0.01)  # Yield control to event loop
                    continue
                
                if chunk is None:  # End of stream
                    if error_container["error"]:
                        yield f"Error: {error_container['error']}"
                    break
                
                yield chunk
                    
        except Exception as e:
            yield f"Error during analysis: {str(e)}"

    async def suggest_move_stream(
        self,
        board_state: Dict,
        move_history: List[Dict] = None,
        model_probs: Dict[str, Dict[int, float]] = None
    ):
        """
        Get a move suggestion with explanation (streaming).
        Uses a queue to properly async iterate over synchronous Gemini stream.
        """
        if not self.client:
            yield "Gemini not configured."
            return
        
        position_text = self._format_position(board_state)
        legal_moves = [m + 1 for m in board_state.get("legal_moves", list(range(9)))]
        ai_data_text = self._format_ai_data(model_probs or {})
        prompt = self._build_suggest_prompt(position_text, legal_moves, ai_data_text)
        
        try:
            gen_cfg = self._build_generate_config(
                max_output_tokens=3000,  # Increased for detailed move suggestions: ~15 paragraphs
                temperature=0.4,
            )
            
            # Use async queue to bridge sync stream to async generator
            import queue
            import threading
            
            chunk_queue: queue.Queue = queue.Queue()
            error_container = {"error": None}
            
            def stream_worker():
                """Worker thread to consume sync stream and put chunks in queue."""
                try:
                    response_stream = self.client.models.generate_content_stream(
                        model=self.model,
                        contents=prompt,
                        config=gen_cfg
                    )
                    for chunk in response_stream:
                        text = self._response_to_text(chunk)
                        if text:
                            chunk_queue.put(text)
                except Exception as e:
                    error_container["error"] = str(e)
                finally:
                    chunk_queue.put(None)  # Signal end of stream
            
            # Start worker thread
            worker = threading.Thread(target=stream_worker, daemon=True)
            worker.start()
            
            # Async consume from queue
            while True:
                try:
                    chunk = chunk_queue.get_nowait()
                except queue.Empty:
                    await asyncio.sleep(0.01)  # Yield control to event loop
                    continue
                
                if chunk is None:  # End of stream
                    if error_container["error"]:
                        yield f"Error: {error_container['error']}"
                    break
                
                yield chunk
                    
        except Exception as e:
            yield f"Error during suggestion: {str(e)}"

    async def voice_conversation_stream(
        self,
        user_query: str,
        previous_analysis: str,
        board_state: Dict,
        move_history: List[Dict] = None
    ):
        """
        Handle voice conversation - user asks a follow-up question about the game.
        Streams response with context of previous analysis.
        """
        if not self.client:
            yield "Gemini –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."
            return
        
        position_text = self._format_position(board_state)
        history_text = self._format_move_history(move_history or [])
        
        prompt = f"""{self.SYSTEM_PROMPT}

‚ïê‚ïê‚ïê –¢–ï–ö–£–©–ê–Ø –ü–û–ó–ò–¶–ò–Ø ‚ïê‚ïê‚ïê
{position_text}

{history_text}

‚ïê‚ïê‚ïê –ü–†–ï–î–´–î–£–©–ò–ô –ê–ù–ê–õ–ò–ó ‚ïê‚ïê‚ïê
{previous_analysis if previous_analysis else "–ê–Ω–∞–ª–∏–∑ –µ—â—ë –Ω–µ –ø—Ä–æ–≤–æ–¥–∏–ª—Å—è."}

‚ïê‚ïê‚ïê –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ‚ïê‚ïê‚ïê
üé§ {user_query}

‚ïê‚ïê‚ïê –ó–ê–î–ê–ù–ò–ï ‚ïê‚ïê‚ïê
–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —É—á–∏—Ç—ã–≤–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –∫–∞—Å–∞–µ—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ö–æ–¥–∞ - –æ–±—ä—è—Å–Ω–∏ –µ–≥–æ.
–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ–±—â–∏–π –æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ - –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π —Å–æ–≤–µ—Ç."""
        
        try:
            gen_cfg = self._build_generate_config(
                max_output_tokens=1500,
                temperature=0.5,
            )
            
            import queue
            import threading
            
            chunk_queue: queue.Queue = queue.Queue()
            error_container = {"error": None}
            
            def stream_worker():
                try:
                    response_stream = self.client.models.generate_content_stream(
                        model=self.model,
                        contents=prompt,
                        config=gen_cfg
                    )
                    for chunk in response_stream:
                        text = self._response_to_text(chunk)
                        if text:
                            chunk_queue.put(text)
                except Exception as e:
                    error_container["error"] = str(e)
                finally:
                    chunk_queue.put(None)
            
            worker = threading.Thread(target=stream_worker, daemon=True)
            worker.start()
            
            while True:
                try:
                    chunk = chunk_queue.get_nowait()
                except queue.Empty:
                    await asyncio.sleep(0.01)
                    continue
                
                if chunk is None:
                    if error_container["error"]:
                        yield f"–û—à–∏–±–∫–∞: {error_container['error']}"
                    break
                
                yield chunk
                    
        except Exception as e:
            yield f"–û—à–∏–±–∫–∞: {str(e)}"

    
    async def comment_move(
        self,
        board_before: Dict,
        board_after: Dict,
        move: int,
        player: str
    ) -> Dict:
        """
        Provide commentary on a move that was just played - in Russian.
        """
        if not self.client:
            return {
                "available": False,
                "error": "Gemini not configured"
            }
        
        # Calculate changes
        if player == "white":
            kazan_before = board_before.get("white_kazan", 0)
            kazan_after = board_after.get("white_kazan", 0)
            player_ru = "–ë–µ–ª—ã–µ"
        else:
            kazan_before = board_before.get("black_kazan", 0)
            kazan_after = board_after.get("black_kazan", 0)
            player_ru = "–ß—ë—Ä–Ω—ã–µ"
        
        kazan_gain = kazan_after - kazan_before
        hole_name = self.HOLE_NAMES.get(move, "")
        
        before_text = self._format_position(board_before)
        after_text = self._format_position(board_after)
        
        prompt = f"""{self.SYSTEM_PROMPT}

‚ïê‚ïê‚ïê –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô –ö –•–û–î–£ ‚ïê‚ïê‚ïê

{player_ru} —Å—ã–≥—Ä–∞–ª–∏ –ª—É–Ω–∫—É {move} ({hole_name}).

–ü–û–ó–ò–¶–ò–Ø –î–û –•–û–î–ê:
{before_text}

–ü–û–ó–ò–¶–ò–Ø –ü–û–°–õ–ï –•–û–î–ê:
{after_text}

–ó–∞—Ö–≤–∞—á–µ–Ω–æ –∫—É–º–∞–ª–∞–∫–æ–≤: {kazan_gain}

–ó–ê–î–ê–ù–ò–ï: –î–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):
1. –ù–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à —ç—Ç–æ—Ç —Ö–æ–¥? (–æ—Ç–ª–∏—á–Ω–æ / —Ö–æ—Ä–æ—à–æ / –Ω–æ—Ä–º–∞–ª—å–Ω–æ / —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ / –æ—à–∏–±–∫–∞)
2. –ß–µ–≥–æ –¥–æ–±–∏–ª—Å—è –∏–≥—Ä–æ–∫ —ç—Ç–∏–º —Ö–æ–¥–æ–º?
3. –ë—ã–ª –ª–∏ –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç?"""

        try:
            gen_cfg = self._build_generate_config(
                max_output_tokens=500,
                temperature=0.4,
            )
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=gen_cfg
                )
            )
            
            text = self._response_to_text(response)
            
            return {
                "available": True,
                "commentary": text,
                "move": move,
                "kazan_gain": kazan_gain
            }
        except Exception as e:
            return {
                "available": False,
                "error": str(e)
            }
    
    async def get_move_probabilities(
        self,
        board_state: Dict
    ) -> Dict[int, float]:
        """
        Get move probabilities (confidence levels) for all legal moves using Gemini.
        Returns a dictionary mapping move (0-8) to probability (0.0-1.0).
        """
        if not self.client:
            return {i: 0.0 for i in range(9)}

        position_text = self._format_position(board_state)
        legal_moves = [m + 1 for m in board_state.get("legal_moves", list(range(9)))]
        legal_with_names = [f"{m} ({self.HOLE_NAMES.get(m, '')})" for m in legal_moves]
        
        prompt = f"""–¢—ã ‚Äî –≥—Ä–æ—Å—Å–º–µ–π—Å—Ç–µ—Ä –¢–æ–≥—ã–∑ –ö—É–º–∞–ª–∞–∫–∞. –û—Ü–µ–Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –∫–∞–∂–¥—ã–π —Ö–æ–¥ —è–≤–ª—è–µ—Ç—Å—è –ª—É—á—à–∏–º.

{position_text}

–î–û–°–¢–£–ü–ù–´–ï –•–û–î–´: {', '.join(legal_with_names)}

–û–¢–í–ï–¢–¨ –°–¢–†–û–ì–û –í JSON-–§–û–†–ú–ê–¢–ï. –ö–ª—é—á–∏ ‚Äî –Ω–æ–º–µ—Ä–∞ –ª—É–Ω–æ–∫, –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (—Å—É–º–º–∞ = 1.0).

–ü—Ä–∏–º–µ—Ä:
{{"3": 0.6, "5": 0.25, "7": 0.15}}

–¢–æ–ª—å–∫–æ JSON, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."""

        try:
            from google.genai import types
            gen_cfg = types.GenerateContentConfig(
                max_output_tokens=200,
                temperature=0.1,
                response_mime_type="application/json"
            )
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=gen_cfg
                )
            )
            
            text = self._response_to_text(response)
            import json
            probs_data = json.loads(text)
            
            # Map back to 0-8 indexing and ensure all 9 pits are covered
            result = {i: 0.0 for i in range(9)}
            for move_str, prob in probs_data.items():
                try:
                    move_idx = int(move_str) - 1
                    if 0 <= move_idx < 9:
                        result[move_idx] = float(prob)
                except (ValueError, TypeError):
                    continue
            
            # Re-normalize if necessary
            total = sum(result.values())
            if total > 0:
                result = {k: v / total for k, v in result.items()}
            else:
                # Fallback to uniform if something went wrong
                if legal_moves:
                    val = 1.0 / len(legal_moves)
                    for m in legal_moves:
                        result[m-1] = val
                        
            return result
            
        except Exception as e:
            print(f"[ERROR] Gemini probabilities failed: {e}")
            # Fallback to uniform
            result = {i: 0.0 for i in range(9)}
            if legal_moves:
                val = 1.0 / len(legal_moves)
                for m in legal_moves:
                    result[m-1] = val
            return result

    def is_available(self) -> bool:
        """Check if Gemini is available."""
        return self.client is not None


# Global analyzer instance
gemini_analyzer = GeminiAnalyzer()
