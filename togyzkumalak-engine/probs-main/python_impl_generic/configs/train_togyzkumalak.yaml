# PROBS Training Config for Togyzkumalak
# Параметры оптимизированы для реального обучения (не для тестов)
# Основано на оригинальном train_chess6x6.yaml с адаптацией под Тогызкумалак

name: train_togyzkumalak
env:
  name: togyzkumalak
  n_max_episode_steps: 200

cmd: train

infra:
  log: tf                    # Логирование в TensorBoard
  device: cpu                # Или 'cuda' для GPU
  sub_processes_cnt: 0       # 0 = без мультипроцессинга, >0 для параллельного Q-обучения
  self_play_threads: 1       # 1 для совместимости с GPU
  mem_max_episodes: 10000    # Размер experience replay буфера

train:
  n_high_level_iterations: 100    # Количество высокоуровневых итераций
  v_train_episodes: 500           # Партий для обучения V-модели (было 2!)
  q_train_episodes: 250           # Партий для обучения Q-модели (было 2!)
  q_dataset_episodes_sub_iter: 1  # Подитераций для Q-датасета
  dataset_drop_ratio: 0.5         # Дропаут для предотвращения переобучения
  checkpoints_dir: checkpoints/togyzkumalak
  train_batch_size: 64            # Размер батча
  self_learning_batch_size: 64    # Батч для self-play
  get_q_dataset_batch_size: 64    # Батч для генерации Q-датасета
  num_q_s_a_calls: 30             # Глубина beam search (кол-во вызовов Q)
  max_depth: 50                   # Максимальная глубина дерева
  alphazero_move_num_sampling_moves: 5   # Первые N ходов сэмплируем (exploration)
  q_add_hardest_nodes_per_step: 5        # Добавляем сложные узлы в датасет

evaluate:
  evaluate_n_games: 20       # Игр для оценки (было 1!)
  randomize_n_turns: 2       # Случайные первые ходы для разнообразия
  enemy:
    kind: one_step_lookahead # Более сильный противник для оценки (было random!)

model:
  value:
    class: ValueModelTK_v1
    learning_rate: 0.0003    # Уменьшен для стабильности (было 0.001)
    weight_decay: 0.0001
  self_learner:
    class: SelfLearningModelTK_v1
    learning_rate: 0.0003    # Уменьшен для стабильности (было 0.001)
    weight_decay: 0.0001
