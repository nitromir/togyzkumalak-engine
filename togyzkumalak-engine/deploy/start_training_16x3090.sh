#!/bin/bash
# ==============================================================================
# Optimized AlphaZero Training for 16x RTX 3090
# ==============================================================================
# Hardware: 16x RTX 3090, AMD EPYC 7543, 434GB RAM
# Expected: ~200-250 iterations per hour
# ==============================================================================

set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  ğŸ¦¾ AlphaZero Training - 16x RTX 3090 Config                 â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Check GPU availability
echo "ğŸ” Checking GPUs..."
python3 << 'EOF'
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}")
print(f"GPUs: {torch.cuda.device_count()}")
for i in range(min(torch.cuda.device_count(), 16)):
    props = torch.cuda.get_device_properties(i)
    print(f"  [{i}] {torch.cuda.get_device_name(i)} - {props.total_memory/1024**3:.1f}GB")
EOF

echo ""
echo "ğŸ“‹ Training Configuration:"
echo "   â€¢ Iterations: 250"
echo "   â€¢ Games/iter: 200"
echo "   â€¢ MCTS sims: 200"
echo "   â€¢ Batch size: 4096 (256 Ã— 16 GPUs)"
echo "   â€¢ Hidden size: 512"
echo "   â€¢ Parallel games: 64"
echo "   â€¢ Bootstrap: enabled"
echo ""

# Confirm
read -p "Start training? (y/n) " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Cancelled."
    exit 0
fi

echo ""
echo "ğŸš€ Starting training via API..."

curl -X POST "http://localhost:8000/api/training/alphazero/start" \
    -H "Content-Type: application/json" \
    -d '{
        "numIters": 250,
        "numEps": 200,
        "numMCTSSims": 200,
        "cpuct": 1.0,
        "batch_size": 4096,
        "hidden_size": 512,
        "epochs": 10,
        "use_bootstrap": true,
        "use_multiprocessing": true,
        "num_parallel_games": 64,
        "save_every_n_iters": 10
    }'

echo ""
echo ""
echo "âœ… Training started!"
echo ""
echo "ğŸ“Š Monitor progress:"
echo "   python deploy/monitor.py"
echo ""
echo "ğŸ’¾ Auto-sync checkpoints (run on LOCAL machine):"
echo "   ./deploy/sync_checkpoints.sh 'root@<ip> -p <port>'"
echo ""
echo "ğŸŒ Web UI: http://localhost:8000"
